{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fellowship.ai One-Shot Learning Challenge (Revision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fellowship challenge, I chose the one-shot learning problem. My reasoning for choosing this challenge is that I would like to get more involved in using AI for image processing and computer vision. This challenge gave me the experience of working with large image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning: Siamese/Relation Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most effective and conceptually simple methods for one-shot learning seem to involve using a \"siamese\" network architecture. The idea is to feed one training image and one test image through a convolutional network with identical parameters. Then the results from both images are fed into another neural network to teach the model to \"compare\" images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original idea applied to one-shot learning was put forth in this paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my project, I used a similar strategy to this paper because it has a more compact structure than the original siamese network: http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create my deep learning model, I used several strategies from the two papers including:\n",
    "\n",
    "-Resizing the images (I chose 26x26 which is approx. 1/4 size)\n",
    "\n",
    "-Distorting the images for training (I used a random affine transform to slightly perturb each image)\n",
    "\n",
    "-Batching several classes at once for each training iteration/episode\n",
    "\n",
    "I also chose to invert the black/white of the images because it works better with the pytorch affine transform function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './SN_Training/siamese_jupyter'\n",
    "TEST_PATH = './SN_Training/siamese_jupyter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:\n",
    "\n",
    "Most of the hyperparameters of the model are the same as the Relation Network paper, however there are a few key differences:\n",
    "\n",
    "-Because sampling 20 classes x 10 examples per class was not feasible with the amount of GPU memory/speed available to me, I decided against it in favor of only sampling 10 classes with 9 examples per class. This also balanced the matching examples with a single mismatched example from each class (9 vs 9) which prevented the network from overfitting to the distribution of classes. A relatively low loss (95% \"correct\") could be obtained by guessing no relation for all classes.\n",
    "\n",
    "-Images were resized to 26x26 and the network architecture was slightly modified to ensure weights fit all sizes.\n",
    "\n",
    "-Trained for 100000 \"episodes\" and stepped the learning rate down every 10000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Hyperparameters\n",
    "###############################################\n",
    "IMG_SIZE = 26\n",
    "HIDDEN_LAYER_SIZE = 8\n",
    "FC_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EX = 9\n",
    "NUM_CL = 10\n",
    "EPISODES = 100000\n",
    "LR_STEP = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notable Helper Functions: \n",
    "I used the bitflip function to change the images so that the character is represented by 1 (white) instead of 0 (black).\n",
    "The \"make target\" function is to return 0 for a mismatched pair and 1 for a matching pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Helper Functions\n",
    "###############################################\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def bitflip(x):\n",
    "    x = torch.mul(x, -1) + 1\n",
    "    return x\n",
    "\n",
    "def make_optimizer(NN_Name, LearningRate):\n",
    "    #input a string representing the name of network\n",
    "    #learning rate should probably be 0.001 or less\n",
    "    optimizer = optim.Adam(NN_Name.parameters(), lr = LearningRate)\n",
    "    return optimizer\n",
    "\n",
    "def make_target(cl1,cl2):\n",
    "    target = cl1==cl2\n",
    "    return target\n",
    "\n",
    "def init_weights(m):\n",
    "    # use by doing $ModuleName$.$layer$.apply(init_weights)\n",
    "    if type(m) == nn.Conv2d:\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif type(m) == nn.BatchNorm1d:\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif type(m) == nn.Linear:\n",
    "        n = m.weight.size(1)\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        m.bias.data = torch.ones(m.bias.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Transform:\n",
    "Pytorch transform function for each image is composed of:\n",
    "1. grayscale (convert to single channel image)\n",
    "2. conversion to numbers followed by the bitflip function\n",
    "3. conversion back to PIL image and randomly affine transformed\n",
    "4. slight crop to 104x104 and resized by 1/4 to 26x26\n",
    "5. finally converted back to numbers and normalized by the mean and stdev of all image pixel values\n",
    "\n",
    "The vis_ transforms are for displaying the images only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Build transforms\n",
    "###############################################\n",
    "\n",
    "# found across the entire dataset pixel values\n",
    "avg_flip = 0.924562 #original black char/white bg\n",
    "avg = 0.075438 #converted to white char/black bg\n",
    "stdev = 0.264097\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.Grayscale(),\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda x: bitflip(x)),\n",
    "                T.ToPILImage(),\n",
    "                T.RandomAffine(15, translate=None, scale=(0.8,1.2), shear=17),\n",
    "                T.CenterCrop(104),\n",
    "                T.Resize((IMG_SIZE,IMG_SIZE),interpolation=1),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([avg], [stdev])\n",
    "            ])\n",
    "\n",
    "transform_no_aff = T.Compose([\n",
    "                T.Grayscale(),\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda x: bitflip(x)),\n",
    "                T.ToPILImage(),\n",
    "                T.CenterCrop(104),\n",
    "                T.Resize((IMG_SIZE,IMG_SIZE),interpolation=1),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([avg], [stdev])\n",
    "            ])\n",
    "\n",
    "vis_transform = T.Compose([\n",
    "                T.Grayscale(),\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda x: bitflip(x)),\n",
    "                T.ToPILImage(),\n",
    "                T.RandomAffine(15, translate=None, scale=(0.8,1.2), shear=17),\n",
    "                T.CenterCrop(104),\n",
    "                T.Resize((IMG_SIZE,IMG_SIZE),interpolation=1),\n",
    "                T.ToTensor()        \n",
    "            ])\n",
    "\n",
    "vis_transform_no_aff = T.Compose([\n",
    "                T.Grayscale(),\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda x: bitflip(x)),\n",
    "                T.ToPILImage(),\n",
    "                T.CenterCrop(104),\n",
    "                T.Resize((IMG_SIZE,IMG_SIZE),interpolation=1),\n",
    "                T.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built a custom pytorch sampler to take a certain number of classes per episode and number of examples per class. This can be modified through the hyperparameters NUM_CL and NUM_EX respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Build Custom Sampler Classes\n",
    "###############################################\n",
    "\n",
    "class SampleSampler(Sampler):\n",
    "    '''Samples 'num_inst' examples each from 'num_cl' groups. \n",
    "    for one shot learning, num_inst is 1 for sample group.\n",
    "    'total_inst' per class, 'total_cl' classes'''\n",
    "\n",
    "    def __init__(self, num_cl=20, total_cl=963, num_inst=1, total_inst=20, shuffle=True):\n",
    "        self.num_cl = num_cl\n",
    "        self.total_cl = total_cl\n",
    "        self.num_inst = num_inst\n",
    "        self.total_inst = total_inst\n",
    "        self.cl_list = list(np.random.choice(total_cl, num_cl, replace=False))\n",
    "        self.ex_list = list(np.random.randint(total_inst, size=num_inst*20))\n",
    "        self.shuffle = shuffle\n",
    "        batch = []\n",
    "        for i, cl in enumerate(self.cl_list):\n",
    "            batch = batch + [20*cl+self.ex_list[i]]\n",
    "        mix = batch[:]\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(mix)\n",
    "        self.batch = batch\n",
    "        self.mix = mix      \n",
    "\n",
    "    def __iter__(self):\n",
    "        # return a single list of indices, assuming that items are grouped 20 per class\n",
    "        if self.shuffle:\n",
    "            return iter(self.mix)\n",
    "        else:\n",
    "            return iter(self.batch)\n",
    "\n",
    "    # the following functions help you retrieve instances\n",
    "    # index of original dataset will be 20*class + example\n",
    "    def get_classes(self):\n",
    "        return self.cl_list\n",
    "\n",
    "    def get_examples(self):\n",
    "        return self.ex_list\n",
    "\n",
    "    def get_batch_idc(self):\n",
    "        return self.batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch)\n",
    "\n",
    "class QuerySampler(Sampler):\n",
    "    '''Samples queries based on class list and example list'''\n",
    "\n",
    "    def __init__(self, cl_list, ex_list, num_inst=19, shuffle=False):\n",
    "        self.cl_list = cl_list\n",
    "        self.ex_list = ex_list\n",
    "        self.num_inst = num_inst\n",
    "        self.shuffle = shuffle\n",
    "        batch = []\n",
    "        for i, cl in enumerate(self.cl_list):\n",
    "            remaining_ex = list(range(20))\n",
    "            remaining_ex.remove(self.ex_list[i])\n",
    "            queries = random.sample(remaining_ex, self.num_inst)\n",
    "            for query in queries:\n",
    "                batch = batch + [20*cl+query]\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(batch)\n",
    "        \n",
    "        self.batch = batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        # return a single list of indices, assuming that items are grouped 20 per class\n",
    "        return iter(self.batch)\n",
    "\n",
    "    # the following functions help you retrieve instances\n",
    "    # index of original dataset will be 20*class + example\n",
    "    def get_classes(self):\n",
    "        return self.cl_list\n",
    "\n",
    "    def get_examples(self):\n",
    "        return self.ex_list\n",
    "\n",
    "    def get_batch_idc(self):\n",
    "        return self.batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#CUDA stuff\n",
    "###############################################\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture: \n",
    "\n",
    "This is the main network which consists of an identical twin network with 4 convolutional layers, each with 64 3x3 filters, batch norm, ReLU, and MaxPool.\n",
    "\n",
    "The 2 outputs of the twin network are concatenated and then fed through a relational network which consists of two more convolutional layers, with the last layer output size 1. Because batchnorm won't work on a 1D feature, I used SELU to try and mimic the effect of normalization. After that the 1D channels are flattened and fed through a fully connected SELU and finally a fully connected sigmoid to give a \"matching probability.\" The output of the first FC layer is size 8 as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Build Siamese Networks\n",
    "###############################################\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, in_channel=1, channel_num=64, FC_num=64, hidden_num=8, output_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channel,channel_num,kernel_size=3,padding=0),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=0.01, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=0.01, affine=True),\n",
    "                        nn.ReLU(),                        \n",
    "                        nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=0.01, affine=True),\n",
    "                        nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=0.01, affine=True),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        # output is 6x6x64 after conv section\n",
    "        self.comp1 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num*2,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=0.01, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        # this is technically a fully connected layer\n",
    "        self.comp2 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=0),\n",
    "                        nn.SELU())\n",
    "        \n",
    "        # input is 64 to FC layer 1\n",
    "        self.fc1 = nn.Sequential(\n",
    "                    nn.Linear(FC_num,hidden_num),\n",
    "                    nn.SELU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "                    nn.Linear(hidden_num,output_size),\n",
    "                    nn.Sigmoid())\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        return out\n",
    "    \n",
    "    def relate(self, x):\n",
    "        out = self.comp1(x)\n",
    "        out = self.comp2(out)\n",
    "        out = flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self,x1,x2):\n",
    "        out1 = self.forward_once(x1)\n",
    "        out2 = self.forward_once(x2)\n",
    "        out_cat = torch.cat((out1,out2),1)\n",
    "        out3 = self.relate(out_cat)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the ImageFolder pytorch class after reorganizing the files and applied the appropriate transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Build Dataset\n",
    "###############################################\n",
    "\n",
    "#training set has 963 classes \n",
    "#test set has has 659 classes\n",
    "#all images were dumped into two folders for training and testing\n",
    "omni_train = dset.ImageFolder(root='./training_images/', transform=transform)\n",
    "omni_train_no_aff = dset.ImageFolder(root='./training_images/', transform=transform_no_aff)\n",
    "omni_test  = dset.ImageFolder(root='./testing_images/', transform=transform_no_aff)\n",
    "omni_vis_train = dset.ImageFolder(root='./training_images/', transform=vis_transform)\n",
    "omni_vis_train_no_aff = dset.ImageFolder(root='./training_images/', transform=vis_transform_no_aff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing:\n",
    "To test the model, I randomly select 20 classes from the testing set and choose 2 examples at random; one for \"training sample\" and one for \"test query.\" Each query is paired with a training sample and a relation score probability is output. The class with the maximum relation score is chosen as the hypothesis. Accuracy is (number correct)/20 for each episode. I thoroughly test the model by going through 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Function for testing the model\n",
    "###############################################\n",
    "\n",
    "def check_accuracy(sample_loader, query_loader, model):\n",
    "    num_correct = 0\n",
    "    num_queries = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        model.to(device=device,dtype=dtype)\n",
    "        for i, (query, query_label) in enumerate(query_loader):\n",
    "            # for each of 20 queries\n",
    "            num_queries += 1\n",
    "            max_score = 0\n",
    "            truth = None\n",
    "            for j, (sample, sample_label) in enumerate(sample_loader):\n",
    "                # check against the 20 one-shot training samples\n",
    "                sample = sample.to(device=device,dtype=dtype)\n",
    "                query = query.to(device=device,dtype=dtype)\n",
    "                score = model(sample, query)\n",
    "                if query_label==sample_label:\n",
    "                    truth = j\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    hypothesis = j\n",
    "\n",
    "            if hypothesis == truth:\n",
    "                num_correct += 1\n",
    "        acc = num_correct/num_queries\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_queries, 100 * acc))\n",
    "        acc_list = [acc]\n",
    "        return acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training:\n",
    "To train the model, I sample some number of classes and examples for each class. Each class pairs up with NUM_EX of its own class and once for each other class. Generally I choose NUM_EX to be NUM_CL - 1 for an equal number of matches and mismatches. I obtained the best performance thus far with 10 classes and 9 examples. Higher numbers were too resource intensive to be able to test, and lower numbers gave slightly less accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Function to Train the model\n",
    "###############################################\n",
    "\n",
    "def train_SN(model, optimizer, scheduler, episodes=1):\n",
    "    \"\"\"Train using siamese network\"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for episode in range(episodes):\n",
    "        scheduler.step(episode)\n",
    "        model.train() # set to train mode\n",
    "\n",
    "        # make the samplers \n",
    "        # make 2 samplers, one for the \"training sample set\" of a one-shot classifier\n",
    "        # other sampler is for the \"test query set\" which provides many comparisons\n",
    "        train_sample_sampler = SampleSampler(num_cl=NUM_CL)\n",
    "        sampled_classes = train_sample_sampler.cl_list\n",
    "        sampled_examples = train_sample_sampler.ex_list\n",
    "        train_query_sampler = QuerySampler(sampled_classes, sampled_examples, num_inst=NUM_EX)\n",
    "\n",
    "        # make the dataloaders\n",
    "        s_batch_num = 1 # one shot \"training\" each\n",
    "        q_batch_num = NUM_EX # pair up number of examples per class in a batch (default 19)\n",
    "        train_sample_loader = DataLoader(omni_train, batch_size=s_batch_num, sampler=train_sample_sampler)\n",
    "        train_query_loader = DataLoader(omni_train, batch_size=q_batch_num, sampler=train_query_sampler)\n",
    "        \n",
    "        # start training\n",
    "        scores = torch.zeros(NUM_CL,(NUM_EX+NUM_CL-1)).to(device=device, dtype=dtype)\n",
    "        targets = torch.zeros(NUM_CL,(NUM_EX+NUM_CL-1)).to(device=device, dtype=dtype)\n",
    "        sample_count = 0\n",
    "        for i, (sample, sample_label) in enumerate(train_sample_loader):\n",
    "            sample_count += 1\n",
    "            idx = 0\n",
    "            for j, (batch, batch_labels) in enumerate(train_query_loader):\n",
    "                if sample_label != batch_labels[0]:\n",
    "                    k = np.random.randint(NUM_EX)\n",
    "                    query = batch[k,:,:,:].to(device=device, dtype=dtype)\n",
    "                    query = query.view(1,1,IMG_SIZE,IMG_SIZE)\n",
    "                    sample = sample.to(device=device, dtype=dtype)\n",
    "                    targets[i,idx] = make_target(sample_label, batch_labels[0])\n",
    "                    scores[i,idx] = model(sample,query)\n",
    "                    idx += 1\n",
    "                    \n",
    "                elif sample_label == batch_labels[0]:\n",
    "                    for k in range(NUM_EX):\n",
    "                        query = batch[k,:,:,:].to(device=device, dtype=dtype)\n",
    "                        query = query.view(1,1,IMG_SIZE,IMG_SIZE)\n",
    "                        sample = sample.to(device=device, dtype=dtype)\n",
    "                        targets[i,idx] = make_target(sample_label, batch_labels[0])\n",
    "                        scores[i,idx] = model(sample,query)\n",
    "                        idx += 1\n",
    "            \n",
    "        targets = targets.view(-1)\n",
    "        scores = scores.view(-1)\n",
    "        \n",
    "        # train and update model\n",
    "        optimizer.zero_grad()\n",
    "        #loss = F.binary_cross_entropy(scores, targets)\n",
    "        loss = F.mse_loss(scores, targets)\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # episodic updates\n",
    "        if (episode+1)%10 == 0:\n",
    "            print(\"episode:\",episode+1,\"loss\",loss.data)\n",
    "\n",
    "        if (episode+1)%100 == 0:\n",
    "            ''' Test the model '''\n",
    "            # make the samplers \n",
    "            test_sample_sampler = SampleSampler(total_cl=659)\n",
    "            sampled_classes = test_sample_sampler.cl_list\n",
    "            sampled_examples = test_sample_sampler.ex_list\n",
    "            test_query_sampler = QuerySampler(sampled_classes, sampled_examples, num_inst=1)\n",
    "\n",
    "            # make the dataloaders\n",
    "            s_batch_num = 1 # one shot each\n",
    "            q_batch_num = 1 # one test each\n",
    "            test_sample_loader = DataLoader(omni_test, batch_size=s_batch_num, sampler=test_sample_sampler)\n",
    "            test_query_loader = DataLoader(omni_test, batch_size=q_batch_num, sampler=test_query_sampler)\n",
    "            check_accuracy(test_sample_loader, test_query_loader, model)\n",
    "\n",
    "        if (episode+1)%1000 == 0:\n",
    "            \"\"\" Save as a draft model \"\"\"\n",
    "            torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Train the net\n",
    "###############################################\n",
    "def TrainTheModel():\n",
    "    snet = SiameseNetwork(FC_num=FC_SIZE,hidden_num=HIDDEN_LAYER_SIZE)\n",
    "    snet.layer1.apply(init_weights)\n",
    "    snet.layer2.apply(init_weights)\n",
    "    snet.layer3.apply(init_weights)\n",
    "    snet.layer4.apply(init_weights)\n",
    "    snet.comp1.apply(init_weights)\n",
    "    snet.comp2.apply(init_weights)\n",
    "    snet.fc1.apply(init_weights)\n",
    "    snet.fc2.apply(init_weights)\n",
    "\n",
    "    snet_optim = make_optimizer(snet,LEARNING_RATE)\n",
    "    snet_scheduler = StepLR(snet_optim,step_size=LR_STEP,gamma=0.5)\n",
    "    print(\"Begin Training...\")\n",
    "    train_SN(snet, snet_optim, snet_scheduler, episodes = EPISODES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Test after saving a model\n",
    "###############################################\n",
    "def TestTheModel(model):\n",
    "    ''' Test the model '''\n",
    "    # make the samplers \n",
    "    test_sample_sampler = SampleSampler(total_cl=659)\n",
    "    sampled_classes = test_sample_sampler.cl_list\n",
    "    sampled_examples = test_sample_sampler.ex_list\n",
    "    test_query_sampler = QuerySampler(sampled_classes, sampled_examples, num_inst=1)\n",
    "\n",
    "    # make the dataloaders\n",
    "    s_batch_num = 1 # one shot each\n",
    "    q_batch_num = 1 # one test each\n",
    "    test_sample_loader = DataLoader(omni_test, batch_size=s_batch_num, sampler=test_sample_sampler)\n",
    "    test_query_loader = DataLoader(omni_test, batch_size=q_batch_num, sampler=test_query_sampler)\n",
    "    return check_accuracy(test_sample_loader, test_query_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program:\n",
    "Running the test program results in around 80% accuracy.\n",
    "Running the training mode will output the loss for each episode, and save a model every 1000 episodes for training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 1 for Train and 2 for Test.\n",
      "2\n",
      "Got 18 / 20 correct (90.00)\n",
      "Got 17 / 20 correct (85.00)\n",
      "Got 19 / 20 correct (95.00)\n",
      "Got 17 / 20 correct (85.00)\n",
      "Got 13 / 20 correct (65.00)\n",
      "Got 13 / 20 correct (65.00)\n",
      "Got 13 / 20 correct (65.00)\n",
      "Got 18 / 20 correct (90.00)\n",
      "Got 17 / 20 correct (85.00)\n",
      "Got 18 / 20 correct (90.00)\n",
      "Got 16 / 20 correct (80.00)\n",
      "Got 15 / 20 correct (75.00)\n",
      "Got 15 / 20 correct (75.00)\n",
      "Got 19 / 20 correct (95.00)\n",
      "Got 18 / 20 correct (90.00)\n",
      "Got 18 / 20 correct (90.00)\n",
      "Got 16 / 20 correct (80.00)\n",
      "Got 14 / 20 correct (70.00)\n",
      "Got 19 / 20 correct (95.00)\n",
      "Got 16 / 20 correct (80.00)\n",
      "Mean (82.25) StDev (10.19)\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#Main Program\n",
    "###############################################\n",
    "if __name__ == '__main__':\n",
    "    program_mode = None\n",
    "    while(program_mode != '1' and program_mode != '2'):\n",
    "        program_mode = input('Press 1 for Train and 2 for Test.\\n')\n",
    "        if program_mode == '1':\n",
    "            TrainTheModel()\n",
    "        elif program_mode == '2':\n",
    "            model = SiameseNetwork(FC_num=FC_SIZE,hidden_num=HIDDEN_LAYER_SIZE)\n",
    "            model.load_state_dict(torch.load(TEST_PATH))\n",
    "            scores = []\n",
    "            for i in range(20):\n",
    "                scores += TestTheModel(model)\n",
    "            avg_score = 100*np.mean(scores)\n",
    "            std_score = 100*np.std(scores, ddof=1)\n",
    "            print('Mean (%.2f) StDev (%.2f)' % (avg_score, std_score))\n",
    "            np.save('scores_jupyter',scores)\n",
    "        else:\n",
    "            print('Sorry, try again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations and Results:\n",
    "\n",
    "The following code blocks will load a random image from the training set and perform an affine transform on it. This allows us to see how the network might be able to recognize the same image despite some distortions.\n",
    "\n",
    "Affine parameters are randomly selected from:\n",
    "[-15, +15] degree rotation;\n",
    "[0.8, 1.2] scale multiplier;\n",
    "[-17, +17] degrees of shear.\n",
    "\n",
    "These are similar to the affine transform from the Siamese Network paper. Shear degrees were calculated through arctan($\\rho$) because the paper uses shear ratios instead of degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_sampler = SampleSampler(num_cl=20, shuffle=False)\n",
    "sampled_classes = train_sample_sampler.cl_list\n",
    "sampled_examples = train_sample_sampler.ex_list\n",
    "train_query_sampler = QuerySampler(sampled_classes, sampled_examples, num_inst=1)\n",
    "\n",
    "s_batch_num = 1 \n",
    "q_batch_num = 1 \n",
    "train_sample_loader = DataLoader(omni_vis_train_no_aff, batch_size=s_batch_num, sampler=train_sample_sampler)\n",
    "train_query_loader = DataLoader(omni_vis_train_no_aff, batch_size=q_batch_num, sampler=train_query_sampler)\n",
    "aff_train_sample_loader = DataLoader(omni_vis_train, batch_size=s_batch_num, sampler=train_sample_sampler)\n",
    "aff_train_query_loader = DataLoader(omni_vis_train, batch_size=q_batch_num, sampler=train_query_sampler)\n",
    "\n",
    "# sample datas\n",
    "aff_sample,aff_sample_labels = aff_train_sample_loader.__iter__().next()\n",
    "aff_query,aff_query_labels = aff_train_query_loader.__iter__().next()\n",
    "sample,sample_labels = train_sample_loader.__iter__().next()\n",
    "query,query_labels = train_query_loader.__iter__().next()\n",
    "\n",
    "# convert to numpy for display\n",
    "aff_sample = aff_sample.view(26,26).numpy()\n",
    "aff_query = aff_query.view(26,26).numpy()\n",
    "sample = sample.view(26,26).numpy()\n",
    "query = query.view(26,26).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fffb7314a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(aff_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fffb770710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "My model resulted in around 81% accuracy with a standard deviation of about 9% for each trial's accuracy. The histogram below shows the scores of the model over 1000 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 81.535%\n",
      "Stdev of scores: 9.37916707389%\n"
     ]
    }
   ],
   "source": [
    "scores = np.load('scores_best.npy')\n",
    "print('Mean score: ' + str(100*scores.mean()) + '%\\nStdev of scores: ' + str(100*scores.std()) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFu1JREFUeJzt3X2UJXV95/H3RxCjIAJOw+IwOOiCi3jYMdsqq9FFMYqggLuisAqjoqM54kMkJoBZcU3Mwfi0MW7wjCsBIyBEILABN8xBgTW7kAwPQQggiCMMzM60gAhi0IHv/lHVyaW5093T93b3TPX7dU6dW/dXT9/qh8+t+7tVt1JVSJK66ynzXYAkaXYZ9JLUcQa9JHWcQS9JHWfQS1LHGfSS1HEGvWZdkjOS/OF816EnS/KCJNcneSjJh+a7Hs0Og34BSrImyfok2/e0vSfJFfNY1kCSvD7JVW1gjSW5MslhW0BdByZZO4vrvyLJewZYxe8CV1TVM6vqS33W/7kkt7c/11uTHDth+rIk1yZ5pH1cNkAtmiUG/cK1LfDh+S5icyXZpk/bW4C/BL4O7AHsBnwCeNMM1r/tdNo65LnAzZNM/znNz/FZwHLgT5K8HCDJdsBFwDeAnYEzgYvadm1JqsphgQ3AGuBE4H5gp7btPTRHdgBLgQK27VnmCuA97fg7gb8Fvgj8FLgTeHnbfjewAVjes+wZwFeAVcBDwJXAc3um/5t22v3AbcBbJyx7GnApTei8dsK+BLgL+Ngk+/sU4PeBH7e1fR141oR9Pa5dz1X92tp5DwD+T7vP/wAc2LONXYA/B+4FHgD+Ctge+AXwOPBwOzynT33Pamsaa2v8feAp7bRPAt/omfeffzfAp4HHgH9q1/3lTez/YTRh/tP297hv2/6dCcvvM42/nYuBE9rx1wH3AOmZfhdw8Hz/jTs8cfCIfuFaTfNP/zszXP5lwI3As4GzgW8CLwH+NfAO4MtJduiZ/+3AHwCLgBuAswDa7qNV7Tp2BY4G/izJfj3L/meaUHsm8L0JdbwAWAJ8a5Ja39kOrwaeB+wAfHnCPP8B2Bd4fb+2JIuBS4A/pAn13wHOTzLSzvsXwDOA/dr9+GJV/Rx4A3BvVe3QDvf2qe9PacL+ee02jwXeNcn+AFBVHwf+N3B8u+7jJ86TZB/gHOAjwAjNC+b/TLJdVb1mwvI/mGx7SZ5O8zsefwewH3BjtQnfurFt1xbEoF/YPgF8sCesNsePqurPq+ox4FyasP1UVT1aVZcBv6QJ/XGXVNVVVfUo8HHg3ydZArwRWNOua2NVXQecD7ylZ9mLqupvq+rxqvqnCXU8u31cN0mtbwe+UFV3VtXDwEnAURO6ZD5ZVT+vql9sou0dwKVVdWlbxyqaF8tDkuxOE+jvr6oHqupXVXXlFD8/4J+7ot4GnFRVD1XVGuDzwDHTWX4a3kbzs19VVb8CPgc8neYd2Ob6Cs07mb9pn+8APDhhngdpXpC1BTHoF7Cqugn4a5punM21vmf8F+36Jrb1HtHf3bPdh2m6aZ5D00f8siQ/HR9ogvlf9Vu2j/vax90nmec5NF0i435M0/Wx2xTb6G17LnDkhDp/o93uEuD+qnpgkho2ZRGwXZ/6Fs9gXf08Yd+r6nGa/dqs9Sf5LPAimm618SP4h4EdJ8y6I033nLYgBr1OAd7LE//xf94+PqOnrTd4Z2LJ+EjbpbMLTX/23cCVVbVTz7BDVf1Wz7KTfcXqbe06/tMk89xLE9Tj9gQ28sQXq37b6G27G/iLCXVuX1WnttN2SbLTFOvo5yfAr/rUd087/nMm/z1Mtf4n7HuS0Pwu7tnkEhMk+a8071heV1U/65l0M7B/u85x+zP5h7uaBwb9AldVd9B0vXyop22MJgjekWSbJO8Gnj/gpg5J8hvtGRl/AFxTVXfTvKPYJ8kxSZ7aDi9Jsu806y/go8B/SfKuJDsmeUq7rZXtbOcAv51kr/ZF5o+Ac6tq42bU/w3gTe1pnNsk+bX21Mk9qmod8G2azxZ2bvfhVe1y64FnJ3nWJup/DDgP+HSSZyZ5brs/32hnuQF4VZI923WcNGEV62n69jflPODQJAcleSpwAvAozYfKU0pyEs1nJL9ZVfdNmHwFzYe5H0rytCTjnxF8Zzrr1twx6AXwKZozRHq9F/gYTdfIfkwzGCZxNs27h/uBf0fTPUNVPURz9sZRNEef/w/4DPC06a64qr5F0xf97nYd62k+NL2oneV0mg9LrwJ+RHOWyQc3p/j2Relw4GSas2Pupvn5jP8PHUNzZH4rzZk9H2mXu5XmhebOtsvnOX1W/0GaI/c7aT5sPrutmfazgHNpPuS8luaFsdefAG9J8kCSJ50HX1W30Xy+8Kc07x7eBLypqn45zV3/I5p3GLcnebgdTm7X/UvgCJoPj39K8/M/YjPWrTmSJ35gLknqGo/oJanjDHpJ6jiDXpI6zqCXpI7bIr6sadGiRbV06dL5LkOStirXXnvtT6pqyivbt4igX7p0KatXr57vMiRpq5Lkx1PPZdeNJHWeQS9JHWfQS1LHGfSS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kddwWcWWspG5aeuIlM152zamHDrGShc2gl7RFmumLhC8QT2bXjSR1nEEvSR03ZdAnWZLku0luSXJzkg+37bskWZXk9vZx57Y9Sb6U5I4kNyb59dneCUnSpk3niH4jcEJV7QscAHwgyQuBE4HLq2pv4PL2OcAbgL3bYQVw2tCrliRN25RBX1Xrquq6dvwh4BZgMXA4cGY725nAEe344cDXq3E1sFOS3YdeuSRpWjarjz7JUuDFwDXAblW1DpoXA2DXdrbFwN09i61t2yaua0WS1UlWj42NbX7lkqRpmXbQJ9kBOB/4SFX9bLJZ+7TVkxqqVlbVaFWNjoxMeScsSdIMTSvokzyVJuTPqqoL2ub1410y7eOGtn0tsKRn8T2Ae4dTriRpc03nrJsAXwNuqaov9Ey6GFjeji8HLuppP7Y9++YA4MHxLh5J0tybzpWxrwCOAb6f5Ia27WTgVOC8JMcBdwFHttMuBQ4B7gAeAd411IolSZtlyqCvqu/Rv98d4KA+8xfwgQHrkiQNiVfGSlLHGfSS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcQa9JHWcQS9JHWfQS1LHGfSS1HEGvSR1nEEvSR03nVsJnp5kQ5KbetrOTXJDO6wZv/NUkqVJftEz7SuzWbwkaWrTuZXgGcCXga+PN1TV28bHk3weeLBn/h9W1bJhFShJGsx0biV4VZKl/aa1Nw5/K/Ca4ZYlSRqWQfvoXwmsr6rbe9r2SnJ9kiuTvHJTCyZZkWR1ktVjY2MDliFJ2pRBg/5o4Jye5+uAPavqxcBHgbOT7NhvwapaWVWjVTU6MjIyYBmSpE2ZcdAn2Rb4j8C5421V9WhV3deOXwv8ENhn0CIlSTM3yBH9a4Fbq2rteEOSkSTbtOPPA/YG7hysREnSIKZzeuU5wP8FXpBkbZLj2klH8cRuG4BXATcm+QfgW8D7q+r+YRYsSdo80znr5uhNtL+zT9v5wPmDlyVJGhavjJWkjjPoJanjDHpJ6jiDXpI6bjrfdSNpgVt64iXzXYIG4BG9JHWcR/SSOmWQdx9rTj10iJVsOTyil6SOM+glqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI6bzh2mTk+yIclNPW2fTHJPkhva4ZCeaScluSPJbUleP1uFS5KmZzpH9GcAB/dp/2JVLWuHSwGSvJDmFoP7tcv82fg9ZCVJ82PKoK+qq4Dp3vf1cOCbVfVoVf0IuAN46QD1SZIGNEgf/fFJbmy7dnZu2xYDd/fMs7Zte5IkK5KsTrJ6bGxsgDIkSZOZadCfBjwfWAasAz7ftqfPvNVvBVW1sqpGq2p0ZGRkhmVIkqYyo6CvqvVV9VhVPQ58lX/pnlkLLOmZdQ/g3sFKlCQNYkZBn2T3nqdvBsbPyLkYOCrJ05LsBewN/N1gJUqSBjHljUeSnAMcCCxKshY4BTgwyTKabpk1wPsAqurmJOcB/whsBD5QVY/NTumSpOmYMuir6ug+zV+bZP5PA58epChJ0vB4ZawkdZxBL0kdZ9BLUscZ9JLUcQa9JHWcQS9JHWfQS1LHGfSS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcVMGfZLTk2xIclNP22eT3JrkxiQXJtmpbV+a5BdJbmiHr8xm8ZKkqU3niP4M4OAJbauAF1XV/sAPgJN6pv2wqpa1w/uHU6YkaaamDPqqugq4f0LbZVW1sX16NbDHLNQmSRqCYfTRvxv4ds/zvZJcn+TKJK/c1EJJViRZnWT12NjYEMqQJPUzUNAn+TiwETirbVoH7FlVLwY+CpydZMd+y1bVyqoararRkZGRQcqQJE1ixkGfZDnwRuDtVVUAVfVoVd3Xjl8L/BDYZxiFSpJmZkZBn+Rg4PeAw6rqkZ72kSTbtOPPA/YG7hxGoZKkmdl2qhmSnAMcCCxKshY4heYsm6cBq5IAXN2eYfMq4FNJNgKPAe+vqvv7rliSNCemDPqqOrpP89c2Me/5wPmDFiVJGh6vjJWkjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp4wx6Seo4g16SOs6gl6SOM+glqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6blpBn+T0JBuS3NTTtkuSVUlubx93btuT5EtJ7khyY5Jfn63iJUlTm+4R/RnAwRPaTgQur6q9gcvb5wBvoLlX7N7ACuC0wcuUJM3UtIK+qq4CJt779XDgzHb8TOCInvavV+NqYKckuw+jWEnS5hukj363qloH0D7u2rYvBu7umW9t2/YESVYkWZ1k9djY2ABlSJImMxsfxqZPWz2poWplVY1W1ejIyMgslCFJgsGCfv14l0z7uKFtXwss6ZlvD+DeAbYjSRrAIEF/MbC8HV8OXNTTfmx79s0BwIPjXTySpLm37XRmSnIOcCCwKMla4BTgVOC8JMcBdwFHtrNfChwC3AE8ArxryDVLkjbDtIK+qo7exKSD+sxbwAcGKUqSNDxeGStJHTetI3pJWgiWnnjJjJZbc+qhQ65kuDyil6SOM+glqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp4wx6Seo4g16SOs6gl6SOM+glqeNm/DXFSV4AnNvT9DzgE8BOwHuBsbb95Kq6dMYVSpIGMuOgr6rbgGUASbYB7gEupLl14Ber6nNDqVCSNJBhdd0cBPywqn48pPVJkoZkWHeYOgo4p+f58UmOBVYDJ1TVAxMXSLICWAGw5557DqkMSZsy07snaes38BF9ku2Aw4C/bJtOA55P062zDvh8v+WqamVVjVbV6MjIyKBlSJI2YRhdN28Arquq9QBVtb6qHquqx4GvAi8dwjYkSTM0jKA/mp5umyS790x7M3DTELYhSZqhgfrokzwD+E3gfT3Nf5xkGVDAmgnTJElzbKCgr6pHgGdPaDtmoIokSUPllbGS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcQa9JHWcQS9JHWfQS1LHGfSS1HEGvSR1nEEvSR1n0EtSxw104xGAJGuAh4DHgI1VNZpkF+BcYCnNXabeWlUPDLotSdLmG9YR/aurallVjbbPTwQur6q9gcvb55KkeTBbXTeHA2e242cCR8zSdiRJUxhG0BdwWZJrk6xo23arqnUA7eOuExdKsiLJ6iSrx8bGhlCGJKmfgfvogVdU1b1JdgVWJbl1OgtV1UpgJcDo6GgNoQ5JUh8DH9FX1b3t4wbgQuClwPokuwO0jxsG3Y4kaWYGCvok2yd55vg48DrgJuBiYHk723LgokG2I0mauUG7bnYDLkwyvq6zq+p/Jfl74LwkxwF3AUcOuB1J0gwNFPRVdSfwb/u03wccNMi6JUnD4ZWxktRxBr0kdZxBL0kdZ9BLUscN44IpSXNo6YmXzHcJ2sp4RC9JHWfQS1LHGfSS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcTMO+iRLknw3yS1Jbk7y4bb9k0nuSXJDOxwyvHIlSZtrkC812wicUFXXtfeNvTbJqnbaF6vqc4OXJ0ka1IyDvqrWAeva8YeS3AIsHlZhkqThGEoffZKlwIuBa9qm45PcmOT0JDtvYpkVSVYnWT02NjaMMiRJfQwc9El2AM4HPlJVPwNOA54PLKM54v98v+WqamVVjVbV6MjIyKBlSJI2YaCgT/JUmpA/q6ouAKiq9VX1WFU9DnwVeOngZUqSZmqQs24CfA24paq+0NO+e89sbwZumnl5kqRBDXLWzSuAY4DvJ7mhbTsZODrJMqCANcD7BqpQkjSQQc66+R6QPpMunXk5krT1GeQ+vmtOPXSIlfTnlbGS1HEGvSR13CB99JJmaJC3+tLm8ohekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp4wx6Seo4L5iSBuCFT9oaeEQvSR1n0EtSx9l1owXP7hd1nUf0ktRxsxb0SQ5OcluSO5KcOFvbkSRNblaCPsk2wH8H3gC8kOb2gi+cjW1JkiY3W330LwXuqKo7AZJ8Ezgc+MdZ2p4msaXf5kzS7JqtoF8M3N3zfC3wst4ZkqwAVrRPH05y2wDbWwT8ZIDltzZztr/5zFxsZVoW2u8Y3OcFIZ8ZaJ+fO52ZZivo+900vJ7wpGolsHIoG0tWV9XoMNa1NVho+wvu80LhPs+O2fowdi2wpOf5HsC9s7QtSdIkZivo/x7YO8leSbYDjgIunqVtSZImMStdN1W1McnxwN8A2wCnV9XNs7Gt1lC6gLYiC21/wX1eKNznWZCqmnouSdJWyytjJanjDHpJ6ritOugX6tcsJNkmyfVJ/nq+a5kLSX47yc1JbkpyTpJfm++ahi3J6Uk2JLmpp+2zSW5NcmOSC5PsNJ81Dlu/fW7bP9j+X9+c5I/nq75hS7IkyXeT3NLu24fb9l2SrEpye/u487C3vdUG/QL/moUPA7fMdxFzIcli4EPAaFW9iObD/aPmt6pZcQZw8IS2VcCLqmp/4AfASXNd1Cw7gwn7nOTVNFfR719V+wGfm4e6ZstG4ISq2hc4APhAm1knApdX1d7A5e3zodpqg56er1moql8C41+z0GlJ9gAOBf7HfNcyh7YFnp5kW+AZdPCajKq6Crh/QttlVbWxfXo1zfUondFvn4HfAk6tqkfbeTbMeWGzpKrWVdV17fhDNAdri2ly68x2tjOBI4a97a056Pt9zcLieaplLv034HeBx+e7kLlQVffQHNXdBawDHqyqy+a3qnnxbuDb813EHNgHeGWSa5JcmeQl813QbEiyFHgxcA2wW1Wtg+bFANh12NvbmoN+yq9Z6JokbwQ2VNW1813LXGn7Kw8H9gKeA2yf5B3zW9XcSvJxmrf9Z813LXNgW2Bnmq6NjwHnJen3v77VSrIDcD7wkar62Vxsc2sO+oX4NQuvAA5Lsoamq+o1Sb4xvyXNutcCP6qqsar6FXAB8PJ5rmnOJFkOvBF4ey2Mi17WAhdU4+9o3rkumueahibJU2lC/qyquqBtXp9k93b67sDQu6u25qBfcF+zUFUnVdUeVbWUZn+/U1VdP7q9CzggyTPaI7uDWDgfRB8M/B5wWFU9Mt/1zJG/Al4DkGQfYDs68m2W7d/v14BbquoLPZMuBpa348uBi4a97a32nrHz8DULmgdVdU2SbwHX0XRfXE8HL5NPcg5wILAoyVrgFJqzbJ4GrGp7L66uqvfPW5FDtol9Ph04vT3l8pfA8g69k3kFcAzw/SQ3tG0nA6fSdFEdR3Ngc+SwN+xXIEhSx23NXTeSpGkw6CWp4wx6Seo4g16SOs6gl6SOM+glqeMMeknquP8PlHPGf79qwwEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ff8c046c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histo = np.histogram(scores, bins=21, range=(0,20))\n",
    "plt.hist(20*scores, bins=21, range=(0,20))\n",
    "plt.title(\"Number Correct out of 20\")\n",
    "plt.xticks(np.arange(0,21,step=4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Future Improvements:\n",
    "Overall, this model achieved decent results for accuracy. However, the model could have better consistency. This could mean that there is room to significantly reduce overfitting of the network. Adding in more rotations/affine transformations of images to augment the existing data could help with this task.\n",
    "\n",
    "I also noticed that downscaling the images may cause some \"blur\" that might affect the efficacy of the convolutional filters. Sending the image values to binary might create sharper edge detection. \n",
    "\n",
    "Regarding network architecture, I think that trying different architectures for the relational portion would be the most effective because it is critical for discerning between images. \n",
    "\n",
    "In addition, the model could be trained for more episodes to reduce overfitting (the Relational Network paper trained for 1 million episodes), and there is much room to experiment with different class/example distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
