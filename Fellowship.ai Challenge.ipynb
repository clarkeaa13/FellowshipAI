{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "import numpy.linalg as alg\n",
    "\n",
    "import copy\n",
    "import metric_learn #need to install first\n",
    "\n",
    "from scipy.ndimage import imread, affine_transform\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from functools import reduce\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fellowship.AI Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Relevant code in the Omniglot repository. This notebook is for short examples and snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fellowship challenge, I chose the one-shot learning problem. \n",
    "My reason for choosing this challenge was that it involved image processing, which is a field that I am interested in but have limited experience with. \n",
    "\n",
    "To attack this problem, I attempted two learning methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first was metric learning using Large Margin Nearest Neighbors(LMNN). This idea came from the fact that the omniglot repository contains an example using a different distance metric than just Euclidean distance. LMNN learns a matrix for maximizing a Mahalanobis distance between datapoints that are dissimilar. I used the following metric-learning codebase as a reference: \n",
    "\n",
    "https://github.com/metric-learn/metric-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind LMNN is to learn a metric such that every data instance is surrounded by at least k members of the same class. However with the small number of examples in the Omniglot dataset, I needed to augment the data. I decided to try to augment each example with a random affine transform, similar to figure 5 in this paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AffineTransImg(I):\n",
    "    # Input: \n",
    "    # Image ndarray pf floats\n",
    "    #\n",
    "    # Output:\n",
    "    # D : [n x 2] rows are coordinates\n",
    "    theta = rng.uniform(-np.pi/36, np.pi/36)\n",
    "    rhox = rng.uniform(-0.2,0.2)\n",
    "    rhoy = rng.uniform(-0.2,0.2)\n",
    "    sx = rng.uniform(0.8,1.2)\n",
    "    sy = rng.uniform(0.8,1.2)\n",
    "    tx = rng.uniform(-2,2)\n",
    "    ty = rng.uniform(-2,2)\n",
    "    c = np.cos(theta)\n",
    "    s = np.sin(theta)\n",
    "    Rot = np.array([[c,s],[-s,c]])\n",
    "    Shr = np. array([[1,rhox],[rhoy,1]])\n",
    "    Sca = np.array([[sx,0],[0,sy]])\n",
    "    A = reduce(np.dot, [Sca, Shr, Rot])\n",
    "    b = np.transpose([[0,0]])\n",
    "    try:\n",
    "        Ainv = alg.inv(A)\n",
    "        HomoA = np.concatenate((Ainv,-np.dot(Ainv,b)),axis=1)\n",
    "        HomoA = np.concatenate((HomoA,[[0,0,1]]))\n",
    "        I = affine_transform(I, Ainv)\n",
    "    except np.linalg.LinAlgError as err:\n",
    "        if 'Singular matrix' in str(err):\n",
    "            pass\n",
    "        else:\n",
    "            raise     \n",
    "    I = downscale_local_mean(I, (3, 3))\n",
    "    I[I >= 0.5] = 1\n",
    "    I[I < 0.5] = 0\n",
    "    I = I.flatten()\n",
    "    return(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The affine transform returns a flattened feature vector for each class, which was then compiled into feature matrix. The learning process of LMNN is done using the metric-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_run(folder,f_load,f_cost,ftype='cost'):\n",
    "    # Compute error rate for one run of one-shot classification\n",
    "    #\n",
    "    # Input\n",
    "    #  folder : contains images for a run of one-shot classification\n",
    "    #  f_load : itemA = f_load('file.png') should read in the image file and process it\n",
    "    #  f_cost : f_cost(itemA,itemB) should compute similarity between two images, using output of f_load\n",
    "    #  ftype  : 'cost' if small values from f_cost mean more similar, or 'score' if large values are more similar\n",
    "    #\n",
    "    # Output\n",
    "    #  perror : percent errors (0 to 100% error)\n",
    "    # \n",
    "    assert ((ftype=='cost') | (ftype=='score'))\n",
    "\n",
    "    # get file names\n",
    "    with open(folder+'/'+fname_label) as f:\n",
    "\t    content = f.read().splitlines()\n",
    "    pairs = [line.split() for line in content]\n",
    "    test_files  = [pair[0] for pair in pairs]\n",
    "    train_files = [pair[1] for pair in pairs]\n",
    "    answers_files = copy.copy(train_files)\n",
    "    test_files.sort()\n",
    "    train_files.sort()\t\n",
    "    ntrain = len(train_files)\n",
    "    ntest = len(test_files)\n",
    "\n",
    "    # load the images (and, if needed, extract features)\n",
    "    train_items = [f_load(f) for f in train_files]\n",
    "    test_items  = [f_load(f) for f in test_files ]\n",
    "\n",
    "    # Augment with 5 affine transforms\n",
    "    # Creates 6 total examples per training item\n",
    "    nexample = 6\n",
    "    feat_mtx = np.zeros((nexample*ntrain,1024),dtype=float)\n",
    "    for i, item in enumerate(train_items):\n",
    "        I = rescale(item, 1.0 / 3.0, anti_aliasing=False)\n",
    "        I = I.astype(bool)\n",
    "        I = I.astype(float)\n",
    "        feat_mtx[(nexample*i),:] = I.flatten()\n",
    "        for j in range(1,nexample):\n",
    "            feat_mtx[(nexample*i)+j,:] = AffineTransImg(item)\n",
    "\n",
    "    # gather the class numbers for each file\n",
    "    classes = np.repeat(np.arange(1,ntrain+1),nexample)\n",
    "\n",
    "    Y = classes\n",
    "    X = feat_mtx\n",
    "\n",
    "    # setting up LMNN\n",
    "    # tried 14 classes because of training data size (want 14 nearest neighbors for each class)\n",
    "    # lowered to k=5 because training takes very long\n",
    "    lmnn = metric_learn.LMNN(k=5, min_iter=50, max_iter=1000, learn_rate=1e-6, regularization=1)\n",
    "\n",
    "    # fit the data\n",
    "    # Use already saved matrices to save time\n",
    "    try:\n",
    "        Minv = np.load(folder+'Minv.npy')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Matrix file not available.\\n\")\n",
    "        print(\"Fitting data...\\n\")\n",
    "        lmnn.fit(X, Y)\n",
    "    # Save Mahalanobis metric matrix as a file for later\n",
    "        Minv = lmnn.metric()\n",
    "        np.save(folder+'Minv.npy', Minv)\n",
    "    \n",
    "    \n",
    "\n",
    "    # compute cost matrix\n",
    "    costM = np.zeros((ntest,ntrain),float)\n",
    "    for i in range(ntest):\n",
    "\t    for c in range(ntrain):\n",
    "\t\t    costM[i,c] = f_cost(test_items[i],train_items[c],Minv)\n",
    "    \n",
    "    if ftype == 'cost':\n",
    "\t    YHAT = np.argmin(costM,axis=1)\n",
    "    elif ftype == 'score':\n",
    "\t    YHAT = np.argmax(costM,axis=1)\n",
    "    else:\n",
    "\t    assert False\n",
    "\n",
    "    # compute the error rate\n",
    "    correct = 0.0\n",
    "    for i in range(ntest):\n",
    "\t    if train_files[YHAT[i]] == answers_files[i]:\n",
    "\t\t    correct += 1.0\n",
    "    pcorrect = 100 * correct / ntest\n",
    "    perror = 100 - pcorrect\n",
    "    return perror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code from the example is used for loading files, etc. Running LMNN as I wrote it resulted in only 10-15% accuracy, which is not much better than random guessing. In order to improve metric learning, I would need to use much larger samples from the omniglot set and more data augmentation via random affine transformations such as rotation, scaling, shearing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to improve these results using deep learning. Several papers have had success on Omniglot with two CNNs running in a comparative manner. I used this one heavily for reference: http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My network architecture was as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, in_channel=1, channel_num=64, hidden_num=64, output_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channel,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "                        nn.Conv2d(channel_num,channel_num,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(channel_num, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        # output is 2x2x64 after conv section\n",
    "\n",
    "        # input is 2x2x64 = 256, output is hidden layer num\n",
    "        self.fc1 = nn.Sequential(\n",
    "                    nn.Linear(channel_num*2*2,hidden_num),\n",
    "                    nn.Sigmoid())\n",
    "\n",
    "        # only do last FC layer after difference function\n",
    "        # input is hidden layer, output is 1 \n",
    "        self.fc2 = nn.Sequential(\n",
    "                    nn.Linear(hidden_num,output_size),\n",
    "                    nn.Sigmoid())\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self,x1,x2):\n",
    "        out1 = self.forward_once(x1)\n",
    "        out2 = self.forward_once(x2)\n",
    "        out_diff = torch.abs(out1-out2)\n",
    "        out3 = self.fc2(out_diff)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network uses 4 convolutional blocks and accepts 2 images for comparison. The last layers are fully connected, with the final step being an absolute difference and a sigmoid activation.\n",
    "\n",
    "I used this architecture because I saw that comparative networks work well for Omniglot, however, I was unable to achieve good results in time. The Relation Network paper uses 1 million training episodes in order to sample the massive dataset size that comes from pairwise-matching of 1623x20 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve my work, I will have to work on ways to avoid overfitting due to the massive number of classes in omniglot. I also want to work on a more powerful machine because all of my work was done locally. Unfortunately this was not a good idea because CNNs and other large learning methods require a lot of computing/GPU power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
